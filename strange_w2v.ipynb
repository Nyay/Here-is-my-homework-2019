{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Решение задачи по поиску ответа на основе корпуса вопросов-отвтев\n",
    "\n",
    "Здесь я использую только word2vec модель для ознакомления работы с этим монстром."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gensim\n",
    "import string\n",
    "import warnings\n",
    "import re\n",
    "import json\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "from pymystem3 import Mystem\n",
    "from gensim import matutils\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from collections import defaultdict\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "mystem = Mystem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Импортируем Word2Vec модель и сам корпус."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_path = '/Users/macbook/Downloads/araneum_none_fasttextcbow_300_5_2018/araneum_none_fasttextcbow_300_5_2018.model'\n",
    "model = Word2Vec.load(model_path)\n",
    "\n",
    "with open('qa_corpus.pkl', 'rb') as file:\n",
    "    qa_corpus = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также я использую стандартную функцию предпроцессинга."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocessing(input_text, del_stopwords=True, del_digit=True):\n",
    "    \"\"\"\n",
    "    :input: raw text\n",
    "        1. lowercase, del punctuation, tokenize\n",
    "        2. normal form\n",
    "        3. del stopwords\n",
    "        4. del digits\n",
    "    :return: lemmas\n",
    "    \"\"\"\n",
    "    russian_stopwords = set(stopwords.words('russian'))\n",
    "    words = [x.lower().strip(string.punctuation+'»«–…') for x in word_tokenize(input_text)]\n",
    "    lemmas = [mystem.lemmatize(x)[0] for x in words if x]\n",
    "\n",
    "    lemmas_arr = []\n",
    "    for lemma in lemmas:\n",
    "        if del_stopwords:\n",
    "            if lemma in russian_stopwords:\n",
    "                continue\n",
    "        if del_digit:\n",
    "            if lemma.isdigit():\n",
    "                continue\n",
    "        lemmas_arr.append(lemma)\n",
    "    return lemmas_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь необходимо написать функцию, которая будет составлять вектор нашего текста, после его подготовки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def w2v_vector(word_list):\n",
    "    some = []\n",
    "    for word in word_list:\n",
    "        try:\n",
    "            some.append(model.wv[str(word)])\n",
    "        except KeyError:\n",
    "            continue\n",
    "    return matutils.unitvec(np.array(sum(some) / len(word_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Составляем словарь, который будет хранить в себе вектор вопроса и текстовый ответ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "index, data = 1, defaultdict()\n",
    "\n",
    "for item in qa_corpus:\n",
    "    data[index] = [w2v_vector(preprocessing(item[0])), item[1]]\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь составим функцию поиска и функцию сравнения векторов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def search(query, result_len):\n",
    "    query_vector = w2v_vector(preprocessing(query))\n",
    "    local_data = defaultdict()\n",
    "    for item in data:\n",
    "        local_data[similarity(query_vector, data[item][0])] = item \n",
    "    for el in sorted(local_data, reverse=True)[:result_len]:\n",
    "        yield data[local_data[el]][1]\n",
    "\n",
    "def similarity(v1, v2):\n",
    "    return np.dot(v1, v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь проведем тесты.\n",
    "\n",
    "Для каждого вопроса в корпусе я составляю его собсвенный вектор и смотрю есть ли соответствующий ответ в топ 5 выдаче поисковика"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For 1384 attempts, there are(is) 1367 correct matches (score:0.9877167630057804)\n"
     ]
    }
   ],
   "source": [
    "true = 0\n",
    "\n",
    "for qa in qa_corpus:\n",
    "    if qa[1] in list(search(qa[0], 5)):\n",
    "        true += 1\n",
    "\n",
    "print('For {} attempts, there are(is) {} correct matches (score:{})'.format(len(qa_corpus), true, true / len(qa_corpus)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
